A from-scratch toy implementation of neural networks, backpropagation, etc

TODO:
* Print out a visual graph in order to debug better
* Fix backward pass/gradient decent
* Implement optimization
* Tidy up code and use the operation wrappers for Tensor
* Fully train a toy Net with only Linear layers
* Implement convolution
* Implement maxpool and avgpool
* Implement BatchNorm1d and 2d
* Test on actual neural networks (full training and evaluation of simple models)

