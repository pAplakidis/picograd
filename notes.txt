DOT PRODUCT

[x1, x2, x3]
dot
[[w1, w2, w3],
 [w4, w5, w6]]
 =
out: 
[[x1*w1 + x2*w2 + x3*w3],
  x1*w4 + x2*w5 * x3*w6]

  
[1, 2, 3]
dot
[[1, 2],
 [3, 4],
 [5, 6]]
= 
[1*1 + 2*3 + 3*5,
 1*2 + 2*4 + 3*6]
=
[22, 28]


TODO: !!!implement this in code for tensor followed by linear layer!!!
backward()
x1.grad += w1.grad * grad(x1*w1)
w1.grad += x1.grad * grad(x1*w1)

grad(x1*w1) += 1.0 * grad(x1*w1 + x2*w2)
grad(x2*w2) += 1.0 * grad(x1*w1 + x2*w2)

out[0].grad = 1.0 * grad(out[0]+b)
b.grad = 1.0 * grad(out[0]+b)

y = w*x + b
θy/θx = w

# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# now suppose we had the gradient on D from above in the circuit
dD = np.random.randn(*D.shape) # same shape as D
dW = dD.dot(X.T) #.T gives the transpose of the matrix
dX = W.T.dot(dD)


