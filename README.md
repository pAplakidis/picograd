# A from-scratch toy implementation of neural networks, backpropagation, etc

## TODO:
* Print out a visual graph in order to debug better (DONE, can be better)
* Fix backward pass/gradient decent (DONE)
* Implement optimization  (DONE)
* Tidy up code and use the operation wrappers for Tensor  (DONE)
* Fully train a toy Net with only Linear layers (DONE)
* Implement convolution
* Implement maxpool and avgpool
* Implement BatchNorm1d and 2d
* Test on actual neural networks (full training and evaluation of simple models)

